{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZudfufEbxev"
      },
      "source": [
        "## Install requirements:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXPEVvgLHT2R",
        "outputId": "f42bed3f-9a1b-4c56-c1e9-bcd25575d174"
      },
      "outputs": [],
      "source": [
        "!git clone 'https://github.com/MSaidKartal/xrays-and-gradcam.git' && cd ../\n",
        "#!pip install -r xrays-and-gradcam/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRGecY22fkCa"
      },
      "source": [
        "# Preamble: Understanding AI in Radiology through Deep Learning\n",
        "\n",
        "Welcome to our interactive workshop designed for radiologists aiming to bridge the gap between artificial intelligence and clinical practice. In this session, we delve into the world of Convolutional Neural Networks (CNNs) and explainable AI, focusing on the application and interpretation of deep learning models in the diagnosis of COVID-19 using chest X-rays.\n",
        "\n",
        "## Workshop Overview\n",
        "\n",
        "Our journey will encompass a hands-on exploration of the key concepts and technologies that are reshaping the landscape of medical imaging:\n",
        "\n",
        "- **Deep Learning Fundamentals**: We begin by demystifying CNNs, the backbone of many modern image recognition tasks, illuminating their inner workings and the principles behind their remarkable capabilities.\n",
        "- **Dataset Preparation**: Understanding and organizing data is crucial. We'll walk through the steps of setting up a dataset for a deep learning project, emphasizing the importance of dividing data into training, validation, and testing sets for a robust model evaluation.\n",
        "- **Model Training and Evaluation**: Step-by-step, we will build, train, and assess a CNN model, examining its performance metrics to understand the implications of model accuracy and reliability in a clinical setting.\n",
        "- **XAI - Explainable AI in Radiology**: We introduce Grad-CAM as a tool for enhancing the transparency of CNN decisions, providing visual explanations that highlight the model's reasoning, and fostering trust in AI-assisted diagnostics.\n",
        "\n",
        "## Our Goals\n",
        "\n",
        "By the end of this workshop, you will:\n",
        "\n",
        "- Acquire a practical understanding of how CNNs can be trained to classify COVID-19 cases from chest X-ray images.\n",
        "- Learn to utilize explainable AI techniques like Grad-CAM to interpret model predictions.\n",
        "- Gain the skills to assess the clinical relevance of AI findings, ensuring the alignment of model focus with the radiologically significant features of the image.\n",
        "\n",
        "We aim to provide a balanced understanding that combines the technical aspects of deep learning with the clinical intuition of radiology, empowering you to harness AI responsibly and effectively in your practice.\n",
        "\n",
        "## Workshop Structure\n",
        "\n",
        "The workshop is structured in a way to encourage interaction, discussion, and hands-on experience:\n",
        "\n",
        "- **Code Snippets**: We'll provide code snippets to illustrate the steps in building and training a CNN, explaining each section's purpose and its place in the larger context of the model's architecture.\n",
        "- **Educational Comments**: Alongside the code, you will find detailed comments that explain the rationale behind the choices made and offer insights into how these steps relate to clinical applications.\n",
        "- **Interactive Sessions**: Participants are encouraged to engage with the material, ask questions, and even experiment with the code to solidify their understanding.\n",
        "\n",
        "We look forward to guiding you through this exciting intersection of radiology and artificial intelligence, equipping you with the knowledge and tools to become an informed user and advocate for AI in medical imaging.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5CJq9xVca4C"
      },
      "source": [
        "# Environment Setup and Data Preprocessing\n",
        "\n",
        "Before diving into the convolutional neural network (CNN) architecture and the training process, it is essential to set up the environment properly and prepare the data for the model. The following code block performs the initial steps necessary for any deep learning task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qUyUhDOXvDu"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages and modules for the deep learning workflow.\n",
        "\n",
        "import os  # Provides functions to interact with the operating system.\n",
        "import torch  # PyTorch, a deep learning framework for tensor computation with GPU acceleration.\n",
        "import torchvision.transforms as transforms  # Transforms are common image transformations.\n",
        "import torchvision.datasets as datasets  # Datasets contains common public datasets.\n",
        "\n",
        "import numpy as np  # NumPy is a library for scientific computing in Python.\n",
        "import matplotlib.pyplot as plt  # Matplotlib's pyplot is used for plotting graphs and images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwlVWaI7cnko"
      },
      "source": [
        "# Download the dataset and the pretrained model\n",
        "\n",
        "I have put the preprocessed version of the dataset and the pretrained model in a public huggingface dataset repository. You can download them with the below commands.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJ8YOu0MHp6l"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('data'): # download data if it doesn't exist\n",
        "    # download data\n",
        "    !wget -q https://huggingface.co/datasets/msaidkartal/xrays-and-gradcam/resolve/main/data.zip\n",
        "    # unzip data\n",
        "    !unzip -q data.zip\n",
        "\n",
        "# -q flag means quiet, so you won't see any output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtTgmmcMYGVa"
      },
      "outputs": [],
      "source": [
        "# Define dictionary paths for the training, validation, and test datasets.\n",
        "# These paths point to the respective directories where the chest X-ray images are stored.\n",
        "dirs = {\n",
        "    'train': 'data/COVID-19_Radiography_Dataset/train',\n",
        "    'val': 'data/COVID-19_Radiography_Dataset/val',\n",
        "    'test': 'data/COVID-19_Radiography_Dataset/test'\n",
        "}\n",
        "\n",
        "# Load the training dataset from the specified directory, applying a transformation to convert images to PyTorch tensors.\n",
        "train_set = datasets.ImageFolder(root=dirs['train'], transform=transforms.ToTensor())\n",
        "\n",
        "# Load the validation dataset in the same manner as the training set.\n",
        "val_set = datasets.ImageFolder(root=dirs['val'], transform=transforms.ToTensor())\n",
        "\n",
        "# Load the test dataset, which will be used to evaluate the model's performance after training.\n",
        "test_set = datasets.ImageFolder(root=dirs['test'], transform=transforms.ToTensor())\n",
        "\n",
        "# Create a DataLoader for the training set, which allows for easy batching, shuffling, and parallel loading using workers.\n",
        "# The batch size is set to 64, which is a common choice for training on moderate-sized datasets.\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W-Y3HhXYKJT",
        "outputId": "08272e6a-44cd-4b76-8fad-8b2652d05754"
      },
      "outputs": [],
      "source": [
        "train_set.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lE8a-9kYL2Q",
        "outputId": "fa2c767b-2005-43f9-9b1f-6b79e8138999"
      },
      "outputs": [],
      "source": [
        "train_set.class_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7se4HO2yBUt",
        "outputId": "8e1fb73b-f21a-4b20-bd23-8a188785e057"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "print(f'Number of Images in Train Set: {len(train_set)}')\n",
        "print(f'Number of Images in Validation Set: {len(val_set)}')\n",
        "print(f'Number of Images in Test Set: {len(test_set)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ke9dwXy0YN5D",
        "outputId": "b82fd527-6586-4a59-d469-add94e5221a0"
      },
      "outputs": [],
      "source": [
        "print(f'Number of Images in Train Set: {len(\"fill here\")}')\n",
        "print(f'Number of Images in Validation Set: {len(\"fill here\")}')\n",
        "print(f'Number of Images in Test Set: {len(\"fill here\")}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZo8lA8Pa3B2"
      },
      "source": [
        "# Sample Image Visualization\n",
        "\n",
        "The code below is designed to show a selection of chest X-ray images from the training set, along with their respective labels. This helps in getting a quick visual confirmation of the data we are using to train the CNN model for COVID classification.\n",
        "\n",
        "- We start by extracting a batch of images and labels from the training loader.\n",
        "- We then convert these images into a displayable format and plot one image for each class label present in the batch.\n",
        "- A figure is generated with subplots—each representing a class of images in the dataset (here, we assume there are four classes, such as COVID-19, normal, viral pneumonia, and bacterial pneumonia).\n",
        "- For each class, we display the first image of that class found in the batch, with the axes turned off for a cleaner look.\n",
        "- The title above each image indicates the class it belongs to, providing clear identification.\n",
        "\n",
        "This visual inspection is crucial to confirm that our dataset contains correctly labeled, diverse, and representative images for each category the CNN will be trained to recognize.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "id": "vw5UMkbPyKLX",
        "outputId": "1251fc8e-6c43-4737-c73b-dee6f959411f"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "for k in range(2):\n",
        "  images, labels = next(iter(train_loader))\n",
        "  images = images.numpy()\n",
        "\n",
        "  # Create a figure by specifying plt.figure, figsize.\n",
        "  fig = plt.figure(figsize=(30, 10))\n",
        "  # Create as many loops as the number of classes, there are 4 classes here.\n",
        "  for i in range(4):  # For classes 0, 1, 2 and 3\n",
        "      # For each class, we get the index of the first image in the labels array\n",
        "      index = np.where(labels == i)[0][0]  # We get the first index for each class\n",
        "\n",
        "      # Create a subplot for each class.\n",
        "      ax = fig.add_subplot(1, 4, i+1)\n",
        "      ax.axis('off')\n",
        "\n",
        "      # Change the axes to display the image properly.\n",
        "      image = np.squeeze(np.transpose(images[index], (1, 2, 0)))\n",
        "\n",
        "      # Draw the image\n",
        "      ax.imshow(image)\n",
        "\n",
        "      # Set titles for each subplot\n",
        "      ax.set_title(train_set.classes[labels[index].item()])\n",
        "\n",
        "  # Show the figure\n",
        "  plt.show()\n",
        "  plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "id": "2K8ZoeMSFd5a",
        "outputId": "0c91fadd-80da-4809-d2d9-387f1024c781"
      },
      "outputs": [],
      "source": [
        "for k in range(2):\n",
        "  images, labels = next(iter(train_loader))\n",
        "  images = images.numpy()\n",
        "\n",
        "  # Create a figure by specifying plt.figure, figsize.\n",
        "  fig = plt.figure(figsize=(30, 10))\n",
        "  # Create as many loops as the number of classes, there are 4 classes here.\n",
        "  for i in range(4):  # For classes 0, 1, 2 and 3\n",
        "      # For each class, we get the index of the first image in the labels array\n",
        "      index = np.where(labels == i)[0][0]  # We get the first index for each class\n",
        "\n",
        "      # Create a subplot for each class.\n",
        "      ax = fig.add_subplot(1, 4, i+1)\n",
        "      ax.axis('off')\n",
        "\n",
        "      # Change the axes to display the image properly.\n",
        "      image = np.squeeze(np.transpose(images[index], (1, 2, 0)))\n",
        "\n",
        "      # Draw the image\n",
        "      ax.imshow(\"fill here\")\n",
        "\n",
        "      # Set titles for each subplot\n",
        "      ax.set_title(train_set.classes[labels[index].item()])\n",
        "\n",
        "  # Show the figure\n",
        "  plt.show()\n",
        "  plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WUGpoq-bDeV"
      },
      "source": [
        "# Dataset Class Distribution Analysis\n",
        "\n",
        "To ensure that our deep learning model is not biased towards any particular class, it is important to analyze the distribution of classes across our datasets. The code snippet provided calculates the frequency of each class label in the training, validation, and test sets.\n",
        "\n",
        "- `freq_train`, `freq_val`, and `freq_test`: These variables store the count of each class in the respective datasets by using the `.bincount()` method on the targets (labels). This method counts the number of occurrences of each value in an array of non-negative ints.\n",
        "- `freq_all`: By summing the individual frequencies from all three sets, we obtain the total frequency of each class across the entire dataset.\n",
        "\n",
        "Understanding the frequency of each class helps in identifying any imbalances that could potentially affect the training outcome. It is crucial for deciding if class weighting or resampling techniques might be necessary to ensure fair representation during the model learning process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7V0seFeYSPf"
      },
      "outputs": [],
      "source": [
        "freq_train = torch.as_tensor(train_set.targets).bincount()\n",
        "freq_val = torch.as_tensor(val_set.targets).bincount()\n",
        "freq_test = torch.as_tensor(test_set.targets).bincount()\n",
        "freq_all = freq_train + freq_test + freq_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPUpyaEWYXyl"
      },
      "outputs": [],
      "source": [
        "def log_freq(dset, dfreq):\n",
        "    for c, i in dset.class_to_idx.items():\n",
        "        print(f'Number of {c.capitalize()} X-Rays: {dfreq[i].item()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07lmHPXPYZ7s",
        "outputId": "628494de-7ac1-434f-9273-387041ede906"
      },
      "outputs": [],
      "source": [
        "print('========== Train Set ==========')\n",
        "log_freq(train_set, freq_train)\n",
        "print('======== Validation Set ========')\n",
        "log_freq(val_set, freq_val)\n",
        "print('=========== Test Set ===========')\n",
        "log_freq(test_set, freq_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "urp4d33-YbqF",
        "outputId": "7faf9443-563c-4938-add6-54d5e298559a"
      },
      "outputs": [],
      "source": [
        "# Apply 'seaborn-notebook' style for consistent and visually appealing plots.\n",
        "plt.style.use('seaborn-notebook')\n",
        "\n",
        "# Create a bar plot showing the distribution of classes (e.g., 'COVID' vs 'Non-COVID') in the training dataset.\n",
        "plt.bar(train_set.classes, freq_all.numpy(), width=0.4)\n",
        "\n",
        "# Iterate over each class frequency to add the exact count above its corresponding bar for clarity.\n",
        "for i, v in enumerate(freq_all.numpy()):\n",
        "    plt.text(i - .2, v + 100, str(v), fontweight='bold')\n",
        "\n",
        "# Label the x-axis as 'Classes' to signify different categories in the dataset.\n",
        "plt.xlabel('Classes', fontweight='bold')\n",
        "# Label the y-axis as 'Number of X-Rays' to reflect the count of images per class.\n",
        "plt.ylabel('Number of X-Rays', fontweight='bold')\n",
        "# Add a title 'Per-class frequency' to describe what the bar plot represents.\n",
        "plt.title('Per-class frequency', fontweight='bold')\n",
        "\n",
        "# Adjust layout to ensure there's no clipping of labels or titles in the plot.\n",
        "plt.tight_layout()\n",
        "# Uncomment the next line if you wish to save the plot as an image file in the specified directory.\n",
        "# plt.savefig('outputs/images/class_freq.png')\n",
        "\n",
        "# Display the plot to show the class frequency distribution.\n",
        "plt.show()\n",
        "# Close the plot to free up memory resources as it is no longer needed after displaying.\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xrcgjOObZce"
      },
      "source": [
        "# Integration of Utility Modules and Grad-CAM\n",
        "\n",
        "To augment our deep learning pipeline with advanced visualization and analysis capabilities, we will include a set of custom utility modules. Additionally, we will integrate Grad-CAM, a technique for highlighting the regions in our input images that are critical for predictions by the CNN.\n",
        "\n",
        "- `sys.path.append(...)`: This line adds the directory containing our custom modules to the system path, making it accessible to Python.\n",
        "- The `utils` and `networks` modules typically contain helper functions and predefined neural network architectures, respectively, which are essential for building and training our models.\n",
        "- `GradCAM` and `plot_utils` import specific functionality for generating Grad-CAM visualizations (`GradCAM`) and plotting confusion matrices and other plots (`plot_confmat`, `plot_gradcam`), which are valuable for interpreting the model's performance and decision-making process.\n",
        "- We re-import `torch` and `torchvision.datasets` as well as `DataLoader` for completeness and to ensure that all necessary components for data handling and model operations are available in the current context.\n",
        "\n",
        "The inclusion of these utilities sets the stage for implementing effective deep learning workflows tailored for radiological image analysis, while also facilitating the interpretability of the model's predictions through visual explanations.\n",
        "\n",
        "This section is streamlined to focus on the relevance of the imported modules and techniques to the core objective of the workshop, ensuring that participants understand their role in the deep learning framework being demonstrated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WcrqPamYeCS"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"xrays-and-gradcam/\")\n",
        "\n",
        "import utils\n",
        "import networks\n",
        "from grad_cam import GradCAM\n",
        "from plot_utils import plot_confmat, plot_gradcam\n",
        "\n",
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELHqd3h6Y5kW"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJT6J2lzbpld"
      },
      "source": [
        "# Preparing Datasets and DataLoaders for Evaluation\n",
        "\n",
        "For the evaluation phase of our deep learning model, it is essential to prepare our validation and test sets with the appropriate transformations, and to set up DataLoaders that will streamline the process of passing data through the model.\n",
        "\n",
        "- The validation (`val_set`) and test (`test_set`) datasets are created using `ImageFolder` with directory paths and transforms defined in the `utils` module. The `transform['eval']` specifies the transformations that should be applied to the datasets during the evaluation phase, such as resizing, normalization, etc., which must be consistent with the transformations applied during training.\n",
        "- `DataLoader` objects for the training, validation, and test sets (`train_dl`, `val_dl`, `test_dl`) are instantiated with a specified `batch_size`. This controls how many images are processed at once during model evaluation. It's important that the batch size fits within your hardware's memory constraints while being large enough for efficient processing.\n",
        "\n",
        "By structuring our data this way, we ensure that the model is evaluated under conditions that match the training phase as closely as possible, allowing for accurate assessment of the model's performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4qZyclaRy9tu"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "val_set = datasets.ImageFolder(root=utils.dirs['val'], transform=utils.transform['eval'])\n",
        "test_set = datasets.ImageFolder(root=utils.dirs['test'], transform=utils.transform['eval'])\n",
        "\n",
        "train_dl = DataLoader(train_set, batch_size=128)\n",
        "val_dl = DataLoader(val_set, batch_size=128)\n",
        "test_dl = DataLoader(test_set, batch_size=120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTjJQEHlyVKR"
      },
      "outputs": [],
      "source": [
        "val_set = datasets.ImageFolder(root=utils.dirs['val'], transform=utils.transform['eval'])\n",
        "test_set = datasets.ImageFolder(root=utils.dirs['test'], transform=utils.transform['eval'])\n",
        "\n",
        "train_dl = DataLoader(\"fill here\", batch_size=128)\n",
        "val_dl = DataLoader(\"fill here\", batch_size=128)\n",
        "test_dl = DataLoader(\"fill here\", batch_size=120)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLMWOWfzrcxk"
      },
      "source": [
        "# Transfer Learning Explained\n",
        "\n",
        "Transfer learning is a powerful technique in deep learning that leverages a pre-trained model as the foundation for a new task. This approach is particularly beneficial when dealing with medical imaging tasks like classifying COVID on chest X-rays, where annotated data can be limited and expensive to acquire.\n",
        "\n",
        "![im](./assets/TL1.png)\n",
        "## Understanding Transfer Learning\n",
        "\n",
        "In the context of Convolutional Neural Networks (CNNs), a pre-trained model is a network that has been previously trained on a large dataset, typically on a related task such as image recognition on a dataset like ImageNet. This model has learned a rich representation of features that can be effectively transferred to a new task with a different dataset.\n",
        "\n",
        "Here’s why transfer learning is pivotal for radiologists:\n",
        "\n",
        "1. **Data Efficiency**: Medical imaging datasets are often smaller compared to the datasets used for training mainstream image recognition models. Transfer learning allows radiologists to work with limited data by utilizing the knowledge the pre-trained model has already acquired.\n",
        "\n",
        "2. **Time and Resource Savings**: Training a CNN from scratch requires significant computational resources and time. Starting with a model that has already learned generic features can significantly reduce the time and resources needed for training.\n",
        "\n",
        "3. **Improved Performance**: Pre-trained models have been optimized to a high degree on the tasks they were initially trained for. When fine-tuned for a specific medical imaging task, they can often outperform models trained from scratch, especially when the available training data is limited.\n",
        "\n",
        "4. **Feature Relevance**: The initial layers of a pre-trained CNN generally capture generic features like edges, textures, and colors. These features are relevant across various tasks, including medical image analysis. The later layers become more specialized. In transfer learning, you can fine-tune these layers to better suit the radiological images and the specific classification task at hand.\n",
        "\n",
        "## Application in Radiology\n",
        "\n",
        "For radiologists, the concept of transfer learning can be analogous to a new radiology resident building upon the foundational medical knowledge acquired in medical school and adapting it to the specialized field of radiology. Just as the resident would start diagnosing cases more effectively over time by learning from specific cases in radiology, a CNN adapts to new imaging tasks by learning from specific features in radiological images.\n",
        "\n",
        "By utilizing transfer learning, radiologists can leverage cutting-edge AI tools without the need to become experts in machine learning. This process allows them to contribute their domain expertise to improve model performance and clinical applicability.\n",
        "\n",
        "In your hands-on workshop, transfer learning will demonstrate how a model trained on general images can adapt to the specialized task of classifying COVID-19 in chest X-rays, showcasing how deep learning can be harnessed to support diagnostic processes in radiology.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDF09009e3Oh"
      },
      "source": [
        "# Insight into VGG16 Architecture\n",
        "\n",
        "VGG16 has been a monumental architecture in the field of deep learning for image classification and detection. Its simplicity and effectiveness have made it a preferred choice for transfer learning, especially in medical imaging where accurate and detailed feature extraction is crucial.\n",
        "\n",
        "![im](./assets/vgg.png)\n",
        "## Overview of VGG16\n",
        "\n",
        "- **Definition**: VGG16 is a convolutional neural network model that excels at object detection and classification, distinguishing among 1000 different categories with a remarkable accuracy rate.\n",
        "- **Nomenclature**: The term 'VGG16' is derived from the Visual Geometry Group at the University of Oxford, indicating that the network is 16 layers deep.\n",
        "- **Origins**: The model was introduced in the seminal paper \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" in 2014, and since then, it has become a staple in the deep learning community.\n",
        "\n",
        "## Characteristics of VGG16\n",
        "\n",
        "- **Architecture**: The architecture of VGG16 is a paragon of simplicity and depth. It consists of 13 convolutional layers with small 3x3 filters, which allows for capturing complex patterns in the image data, and 3 fully connected layers at the end of the network.\n",
        "- **Depth**: The '16' in VGG16 refers to the total count of convolutional and fully connected layers that have trainable weights.\n",
        "- **Performance**: Upon its release, VGG16 made headlines by achieving top-tier performance on the ImageNet challenge, which is a highly regarded benchmark for image classification and vision-related tasks.\n",
        "\n",
        "## Importance for Radiology\n",
        "\n",
        "For the radiology community, VGG16 is more than just a deep learning architecture; it represents a significant development in the field of computer-aided diagnosis. The key features of VGG16, such as its deep architecture and the use of small convolutional filters, allow it to extract detailed features from medical images, which is vital for accurate diagnosis. When used as a starting point for transfer learning, VGG16 can be fine-tuned to work with specific imaging data in radiology, such as chest X-rays, to classify conditions with high accuracy.\n",
        "\n",
        "In our hands-on workshop, you will explore how to utilize VGG16’s transfer learning capabilities to address the challenges of COVID-19 classification in chest X-ray images, highlighting how historical advancements in AI can be directly applied to modern medical challenges.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZlOptaj0zUGQ"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Load a pre-trained VGG16 model with custom output features, specifically tailored for the dataset at hand.\n",
        "# Here, 'out_features=4' suggests we are classifying the images into 4 distinct classes.\n",
        "vgg16 = networks.get_vgg16(out_features=4, path='models/lr3e-5_vgg_cuda.pth')\n",
        "\n",
        "# Use a utility function to get predictions from the validation dataset using the loaded VGG16 model.\n",
        "val_preds = utils.get_all_preds(vgg16, val_dl)\n",
        "# Similarly, obtain predictions from the test dataset to evaluate the model's performance.\n",
        "test_preds = utils.get_all_preds(vgg16, test_dl)\n",
        "\n",
        "# Process the predictions from the test dataset to obtain the most likely class for each X-ray.\n",
        "# The '.cpu()' method is called to move the data from the GPU to CPU memory if CUDA is used for computation.\n",
        "# The 'argmax(dim=1)' function finds the class with the highest predicted probability for each image.\n",
        "vgg_preds = test_preds.cpu().argmax(dim=1)\n",
        "\n",
        "# Print out the shape of the predictions for both validation and test sets.\n",
        "# This is useful for verification and understanding the dimensions of the model's outputs.\n",
        "val_preds.shape, test_preds.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mls4tHgBZNVo",
        "outputId": "9003325e-4be9-456d-cb20-96ff59bd914f"
      },
      "outputs": [],
      "source": [
        "# Load a pre-trained VGG16 model with custom output features, specifically tailored for the dataset at hand.\n",
        "# Here, 'out_features=4' suggests we are classifying the images into 4 distinct classes.\n",
        "vgg16 = networks.get_vgg16(out_features=4, path='models/lr3e-5_vgg_cuda.pth')\n",
        "\n",
        "# Use a utility function to get predictions from the validation dataset using the loaded VGG16 model.\n",
        "val_preds = utils.get_all_preds(\"fill here\", val_dl)\n",
        "# Similarly, obtain predictions from the test dataset to evaluate the model's performance.\n",
        "test_preds = utils.get_all_preds(\"fill here\", test_dl)\n",
        "\n",
        "# Process the predictions from the test dataset to obtain the most likely class for each X-ray.\n",
        "# The '.cpu()' method is called to move the data from the GPU to CPU memory if CUDA is used for computation.\n",
        "# The 'argmax(dim=1)' function finds the class with the highest predicted probability for each image.\n",
        "vgg_preds = test_preds.cpu().argmax(dim=1)\n",
        "\n",
        "# Print out the shape of the predictions for both validation and test sets.\n",
        "# This is useful for verification and understanding the dimensions of the model's outputs.\n",
        "val_preds.shape, test_preds.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tpSRuKy1zc_Y"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Calculate the number of correct predictions in the validation set by comparing the model's predictions\n",
        "# with the actual targets/labels. The predictions are moved to the same device as the targets for comparison.\n",
        "val_correct = utils.get_num_correct(val_preds, torch.as_tensor(val_set.targets, device=device))\n",
        "\n",
        "# Calculate the number of correct predictions in the test set in the same manner as for the validation set.\n",
        "test_correct = utils.get_num_correct(test_preds, torch.as_tensor(test_set.targets, device=device))\n",
        "\n",
        "# Print the number of correctly predicted images and the accuracy percentage for the validation set.\n",
        "# The accuracy is calculated by dividing the number of correct predictions by the total number of images\n",
        "# and then converting it to a percentage.\n",
        "print(f'Validation Correct: {val_correct:5}\\tValidation Accuracy: {(100*val_correct/len(val_set)):5.2f}%')\n",
        "\n",
        "# Print the number of correctly predicted images and the accuracy percentage for the test set.\n",
        "# The formatting ensures that the output is easily readable and provides quick insight into model performance.\n",
        "print(f'Test Correct: {test_correct:6}\\tTest Accuracy: {(100*test_correct/len(test_set)):6.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abutnYTBZOio",
        "outputId": "30cb37df-773a-49a8-ee9a-89c2fc02de94"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of correct predictions in the validation set by comparing the model's predictions\n",
        "# with the actual targets/labels. The predictions are moved to the same device as the targets for comparison.\n",
        "val_correct = utils.get_num_correct(\"fill here\", torch.as_tensor(val_set.targets, device=device))\n",
        "\n",
        "# Calculate the number of correct predictions in the test set in the same manner as for the validation set.\n",
        "test_correct = utils.get_num_correct(\"fill here\", torch.as_tensor(test_set.targets, device=device))\n",
        "\n",
        "# Print the number of correctly predicted images and the accuracy percentage for the validation set.\n",
        "# The accuracy is calculated by dividing the number of correct predictions by the total number of images\n",
        "# and then converting it to a percentage.\n",
        "print(f'Validation Correct: {val_correct:5}\\tValidation Accuracy: {(100*val_correct/len(val_set)):5.2f}%')\n",
        "\n",
        "# Print the number of correctly predicted images and the accuracy percentage for the test set.\n",
        "# The formatting ensures that the output is easily readable and provides quick insight into model performance.\n",
        "print(f'Test Correct: {test_correct:6}\\tTest Accuracy: {(100*test_correct/len(test_set)):6.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "b-viD2h1affk"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "val_confmat = utils.get_confmat(val_set.targets, val_preds)\n",
        "test_confmat = utils.get_confmat(test_set.targets, test_preds)\n",
        "plot_confmat(val_confmat, test_confmat, train_set.classes, f'{type(vgg16).__name__.lower()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3joPm68zkm5"
      },
      "outputs": [],
      "source": [
        "val_confmat = utils.get_confmat(val_set.targets, \"fill here\")\n",
        "test_confmat = utils.get_confmat(test_set.targets, \"fill here\")\n",
        "plot_confmat(val_confmat, test_confmat, train_set.classes, f'{type(vgg16).__name__.lower()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVCHHhiAafdY",
        "outputId": "33e47a1e-16db-42bf-88e8-7570688b90a8"
      },
      "outputs": [],
      "source": [
        "results = utils.get_results(test_confmat, test_set.classes)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7-0JZ5QjRvX"
      },
      "source": [
        "# Delving into ResNet-18 Architecture\n",
        "\n",
        "ResNet-18 is a prominent architecture within the deep learning landscape, especially recognized for its innovative approach to handling deeper neural networks through the concept of residual learning.\n",
        "\n",
        "![im](./assets/resn.png)\n",
        "## ResNet-18 At a Glance\n",
        "\n",
        "- **Denotation**: The '18' in ResNet-18 signifies the number of layers it contains, making it a relatively more accessible model within the ResNet family, suitable for a range of computational environments.\n",
        "- **Inception**: This model was conceived by researchers at Microsoft and shared with the broader academic and professional community through the pivotal paper \"Deep Residual Learning for Image Recognition\" in 2015.\n",
        "- **Architecture Fundamentals**: Belonging to the ResNet (Residual Network) series, ResNet-18 implements an innovative design that incorporates 'skip connections'. These connections effectively allow the input of one layer to \"skip\" computing layers and flow directly into deeper layers within the network.\n",
        "\n",
        "## The Innovation of Skip Connections\n",
        "\n",
        "- **Solving Vanishing Gradients**: The skip connections are a breakthrough that mitigates the vanishing gradient issue—a common obstacle in training very deep networks—by allowing gradients to flow through the network without attenuation during backpropagation.\n",
        "- **Enabling Deeper Networks**: With these residual blocks, ResNet-18 can enjoy the benefits of deeper architectural complexity without the hindrance of training difficulties that typically plague such depth.\n",
        "\n",
        "## Impact on Radiology\n",
        "\n",
        "For the radiology sector, ResNet-18's architecture provides an excellent foundation for developing highly accurate diagnostic models. The efficiency of training and the model's adeptness at feature extraction due to its depth make it a valuable tool for medical image analysis, such as in detecting pathologies in chest X-rays or other radiographic imaging.\n",
        "\n",
        "In the context of your workshop, ResNet-18 serves as a prime example of how advancements in deep learning architecture can facilitate powerful diagnostic aids. The participants will engage with ResNet-18 to understand how it can be utilized for the classification of COVID-19 from chest X-rays, thereby leveraging deep learning to potentially improve patient outcomes and radiological practices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGEzPbpXzsS1",
        "outputId": "5ada3972-598e-483f-e8ac-d4687b0b9c1f"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Initialize a pre-trained ResNet18 model from a saved file, adjusting the final layer to predict 4 output classes.\n",
        "# This could correspond to different diagnoses or findings in the chest X-rays.\n",
        "resnet18 = networks.get_resnet18(out_features=4, path='models/lr3e-5_resnet_cuda.pth')\n",
        "\n",
        "# Obtain predictions from the ResNet18 model for all the images in the validation dataset loader.\n",
        "# This is an essential step in evaluating the model's performance on unseen data.\n",
        "val_preds = utils.get_all_preds(resnet18, val_dl)\n",
        "\n",
        "# Similarly, generate predictions for all the images in the test dataset loader to further evaluate the model.\n",
        "test_preds = utils.get_all_preds(resnet18, test_dl)\n",
        "\n",
        "# Process the predictions from the test dataset by selecting the class with the highest probability.\n",
        "# The use of '.cpu()' is necessary to transfer tensors from GPU to CPU if the model computations are done on a GPU.\n",
        "resnet_preds = test_preds.cpu().argmax(dim=1)\n",
        "\n",
        "# Output the shape of the prediction tensors for validation and test datasets.\n",
        "# The shape will confirm the number of predictions made and if they align with the number of input samples.\n",
        "val_preds.shape, test_preds.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQgpxZbZafax",
        "outputId": "5a091663-0088-4c25-f2bc-5d8094f96fea"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Initialize a pre-trained ResNet18 model from a saved file, adjusting the final layer to predict 4 output classes.\n",
        "# This could correspond to different diagnoses or findings in the chest X-rays.\n",
        "resnet18 = networks.get_resnet18(out_features=4, path='models/lr3e-5_resnet_cuda.pth')\n",
        "\n",
        "# Obtain predictions from the ResNet18 model for all the images in the validation dataset loader.\n",
        "# This is an essential step in evaluating the model's performance on unseen data.\n",
        "val_preds = utils.get_all_preds(\"fill here\", val_dl)\n",
        "\n",
        "# Similarly, generate predictions for all the images in the test dataset loader to further evaluate the model.\n",
        "test_preds = utils.get_all_preds(\"fill here\", test_dl)\n",
        "\n",
        "# Process the predictions from the test dataset by selecting the class with the highest probability.\n",
        "# The use of '.cpu()' is necessary to transfer tensors from GPU to CPU if the model computations are done on a GPU.\n",
        "resnet_preds = test_preds.cpu().argmax(dim=1)\n",
        "\n",
        "# Output the shape of the prediction tensors for validation and test datasets.\n",
        "# The shape will confirm the number of predictions made and if they align with the number of input samples.\n",
        "val_preds.shape, test_preds.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wqrzkh69afX3",
        "outputId": "2fcc23c1-8c03-4abe-ef63-47efa89da64d"
      },
      "outputs": [],
      "source": [
        "val_correct = utils.get_num_correct(val_preds, torch.as_tensor(val_set.targets, device=device))\n",
        "test_correct = utils.get_num_correct(test_preds, torch.as_tensor(test_set.targets, device=device))\n",
        "\n",
        "print(f'Validation Correct: {val_correct:5}\\tValidation Accuracy: {(100*val_correct/len(val_set)):5.2f}%')\n",
        "print(f'Test Correct: {test_correct:6}\\tTest Accuracy: {(100*test_correct/len(test_set)):6.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pvKxnVOqz14M"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "val_confmat = utils.get_confmat(val_set.targets, val_preds)\n",
        "test_confmat = utils.get_confmat(test_set.targets, test_preds)\n",
        "plot_confmat(val_confmat, test_confmat, train_set.classes, f'{type(vgg16).__name__.lower()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "3a3-lgfgafVj",
        "outputId": "e1d95875-04a2-4cde-f7a6-4603c1fbfdce"
      },
      "outputs": [],
      "source": [
        "val_confmat = utils.get_confmat(val_set.targets, \"fill here\")\n",
        "test_confmat = utils.get_confmat(test_set.targets, \"fill here\")\n",
        "plot_confmat(val_confmat, test_confmat, train_set.classes, f'{type(vgg16).__name__.lower()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vzws1-TgafTE",
        "outputId": "541d142c-eb73-43e7-ef51-e4c9f92135af"
      },
      "outputs": [],
      "source": [
        "results = utils.get_results(test_confmat, test_set.classes)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQg9SXAdjzE4"
      },
      "source": [
        "# Exploring DenseNet-121 Architecture\n",
        "\n",
        "DenseNet-121 stands out in the realm of deep learning architectures due to its unique design and efficiency, particularly in the context of handling complex image data such as medical imaging.\n",
        "\n",
        "![im](./assets/dens.png)\n",
        "## DenseNet-121 Overview\n",
        "\n",
        "- **Naming Convention**: The \"121\" in DenseNet121 indicates the number of neural layers that comprise the network, highlighting the depth and complexity of the model.\n",
        "- **Origins**: The conceptualization of DenseNet121 was first introduced in the research paper \"Densely Connected Convolutional Networks\" in 2017, marking a significant step forward in convolutional network design.\n",
        "- **Architectural Innovations**: DenseNet architectures, including DenseNet121, are celebrated for their dense connectivity pattern. Every layer directly connects to every other layer that follows, creating a densely connected web of layers within the network.\n",
        "\n",
        "## The Dense Connectivity Paradigm\n",
        "\n",
        "- **Parameter Efficiency**: The dense connections of DenseNet121 significantly reduce the model's parameters compared to other networks with a similar depth. This efficiency enables maintaining or even enhancing model performance while being more economical with resources.\n",
        "- **Enhanced Feature Propagation**: With each layer receiving input from all preceding layers, there's a richer feature set available at each point, improving information flow and feature reuse throughout the network.\n",
        "- **Combatting Overfitting**: The inherent feature-rich connectivity of DenseNet121 also helps in reducing overfitting, a common challenge when training deep networks, especially on limited datasets.\n",
        "\n",
        "## Relevance to Radiology\n",
        "\n",
        "For radiologists, DenseNet-121 offers a powerful tool for medical image classification. Its parameter efficiency and deep structure are well-suited for capturing the nuanced patterns present in complex radiographic images. When applied to chest X-ray analysis, DenseNet-121 could enhance the accuracy of identifying and classifying pathologies, including those associated with COVID-19, which may be subtle and dispersed across the image.\n",
        "\n",
        "The participants of your workshop will gain insights into how DenseNet-121's architecture is particularly advantageous in the medical field. By examining its use for COVID-19 classification in chest X-rays, they will learn about the cutting-edge of deep learning technology that has the potential to revolutionize medical diagnostics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H18b0MGI0AaD",
        "outputId": "d1c96f2d-ed09-4c38-e602-ee88dc07a161"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Initialize a DenseNet121 model tailored for the task with 4 output features (e.g., classification categories).\n",
        "# The model weights are loaded from a saved path, indicating a pre-trained state or previous training session.\n",
        "densenet121 = networks.get_densenet121(out_features=4, path='models/lr3e-5_densenet_cuda.pth')\n",
        "\n",
        "# Generate predictions on the validation dataset using the DenseNet121 model. This step is critical for\n",
        "# preliminary assessment of the model's performance on data it hasn't been trained on.\n",
        "val_preds = utils.get_all_preds(densenet121, val_dl)\n",
        "\n",
        "# Repeat the prediction process for the test dataset. Comparing validation and test predictions can\n",
        "# help in understanding how well the model generalizes to completely unseen data.\n",
        "test_preds = utils.get_all_preds(densenet121, test_dl)\n",
        "\n",
        "# Extract the predicted class indices from the test predictions by taking the argmax,\n",
        "# which represents the most likely class according to the model's output.\n",
        "densenet_preds = test_preds.cpu().argmax(dim=1)\n",
        "\n",
        "# Display the shapes of the prediction tensors for both the validation and test sets.\n",
        "# This information confirms the number of predictions and ensures they correspond to\n",
        "# the number of samples in each respective dataset.\n",
        "val_preds.shape, test_preds.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z06K16uOafQ-",
        "outputId": "5301d6b1-ec45-4ceb-fdb8-44e3792627ec"
      },
      "outputs": [],
      "source": [
        "# Initialize a DenseNet121 model tailored for the task with 4 output features (e.g., classification categories).\n",
        "# The model weights are loaded from a saved path, indicating a pre-trained state or previous training session.\n",
        "densenet121 = networks.get_densenet121(out_features=4, path='models/lr3e-5_densenet_cuda.pth')\n",
        "\n",
        "# Generate predictions on the validation dataset using the DenseNet121 model. This step is critical for\n",
        "# preliminary assessment of the model's performance on data it hasn't been trained on.\n",
        "val_preds = utils.get_all_preds(\"fill here\", val_dl)\n",
        "\n",
        "# Repeat the prediction process for the test dataset. Comparing validation and test predictions can\n",
        "# help in understanding how well the model generalizes to completely unseen data.\n",
        "test_preds = utils.get_all_preds(\"fill here\", test_dl)\n",
        "\n",
        "# Extract the predicted class indices from the test predictions by taking the argmax,\n",
        "# which represents the most likely class according to the model's output.\n",
        "densenet_preds = test_preds.cpu().argmax(dim=1)\n",
        "\n",
        "# Display the shapes of the prediction tensors for both the validation and test sets.\n",
        "# This information confirms the number of predictions and ensures they correspond to\n",
        "# the number of samples in each respective dataset.\n",
        "val_preds.shape, test_preds.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIIgpkLPafNy",
        "outputId": "0f80035d-6411-4333-852a-b059fcc50e34"
      },
      "outputs": [],
      "source": [
        "val_correct = utils.get_num_correct(val_preds, torch.as_tensor(val_set.targets, device=device))\n",
        "test_correct = utils.get_num_correct(test_preds, torch.as_tensor(test_set.targets, device=device))\n",
        "\n",
        "print(f'Validation Correct: {val_correct:5}\\tValidation Accuracy: {(100*val_correct/len(val_set)):5.2f}%')\n",
        "print(f'Test Correct: {test_correct:6}\\tTest Accuracy: {(100*test_correct/len(test_set)):6.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pW5LdAiW0FPx"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "val_confmat = utils.get_confmat(val_set.targets, val_preds)\n",
        "test_confmat = utils.get_confmat(test_set.targets, test_preds)\n",
        "plot_confmat(val_confmat, test_confmat, train_set.classes, f'{type(vgg16).__name__.lower()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "l6ten1NQafFe",
        "outputId": "253fcd07-bf14-45a7-d44b-13cab09d9d30"
      },
      "outputs": [],
      "source": [
        "val_confmat = utils.get_confmat(val_set.targets, \"fill here\")\n",
        "test_confmat = utils.get_confmat(test_set.targets, \"fill here\")\n",
        "plot_confmat(val_confmat, test_confmat, train_set.classes, f'{type(vgg16).__name__.lower()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA2K29kQa9Zw",
        "outputId": "afc14856-3fa4-4337-8ccb-1530f8871242"
      },
      "outputs": [],
      "source": [
        "results = utils.get_results(test_confmat, test_set.classes)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPLZjo6gkEbU"
      },
      "source": [
        "# Unveiling Model Decisions with Grad-CAM\n",
        "\n",
        "Grad-CAM, short for Gradient-weighted Class Activation Mapping, is a powerful interpretability tool that provides insights into the inner workings of Convolutional Neural Networks (CNNs). In medical imaging, understanding the model's focus is crucial for diagnosis verification and gaining clinician trust.\n",
        "\n",
        "![im](./assets/grad1.png)\n",
        "![im](./assets/grad2.png)\n",
        "\n",
        "## Understanding Grad-CAM\n",
        "\n",
        "- **Purpose**: Grad-CAM aids in illuminating the \"why\" behind a model's decision, offering a heatmap visualization of the areas within an image that are influential in the model's prediction process.\n",
        "- **Process**: It leverages the gradients flowing into the final convolutional layer of a CNN to understand each neuron's importance for a decision of interest. These gradients are then mapped back onto the input space, producing a heatmap corresponding to the significant regions for the prediction.\n",
        "- **Transparency and Trust**: By revealing which parts of an image the model is focusing on to make a decision, Grad-CAM demystifies the CNN's operation, fostering transparency and trust in its diagnostic recommendations.\n",
        "\n",
        "## Grad-CAM in Radiology\n",
        "\n",
        "For the radiologist, Grad-CAM is a step towards explainable AI in medical diagnostics. It can be particularly enlightening when applied to chest X-ray analysis:\n",
        "\n",
        "- **Highlighting Pathological Features**: Grad-CAM heatmaps can highlight areas of a chest X-ray that signify pathological features, such as opacities or consolidations associated with conditions like COVID-19.\n",
        "- **Educational Value**: These visualizations serve as an educational tool, enabling radiologists to correlate the CNN's findings with their knowledge and experience, thus reinforcing the learning process.\n",
        "- **Clinical Relevance**: Grad-CAM can be instrumental in validating the model's focus, ensuring that it aligns with clinically relevant features in the image rather than artifacts or noise.\n",
        "\n",
        "This section of the workshop will dive into Grad-CAM's methodology and its application in the context of COVID-19 chest X-ray classification. Participants will gain practical experience in interpreting Grad-CAM heatmaps, enriching their understanding of how AI models can complement their expertise in diagnosing and understanding the disease manifestations on radiological images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vJLP6rgt0QeN"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Select an X-ray image and its true label from the test set using an index.\n",
        "# The chosen image will be used for visualization of class activation maps (CAMs).\n",
        "index = 88\n",
        "image, label = test_set[index]\n",
        "\n",
        "# Retrieve the class names corresponding to the true label and the predicted labels from each model\n",
        "# for the selected image. This aids in understanding the model's predictions in human-readable terms.\n",
        "label_name = [\n",
        "    test_set.classes[label],  # True label\n",
        "    test_set.classes[vgg_preds[index]],  # Prediction from VGG16\n",
        "    test_set.classes[resnet_preds[index]],  # Prediction from ResNet18\n",
        "    test_set.classes[densenet_preds[index]]  # Prediction from DenseNet121\n",
        "]\n",
        "\n",
        "# Prepare the image for the model by adding a batch dimension and transferring it to the same device as the model.\n",
        "image = image.unsqueeze(dim=0).to(device)\n",
        "\n",
        "# Initialize the Grad-CAM object for VGG16 and generate the CAM for the selected image and its true label.\n",
        "# The target layer is the final convolutional layer of the VGG16 model.\n",
        "cam_obj = GradCAM(model=vgg16, target_layer=vgg16.features[-1])\n",
        "_, vgg_cam = cam_obj(image, label)\n",
        "\n",
        "# Repeat the process with the ResNet18 model, targeting the final layer of the model's fourth layer block.\n",
        "cam_obj = GradCAM(model=resnet18, target_layer=resnet18.layer4[-1])\n",
        "_, res_cam = cam_obj(image, label)\n",
        "\n",
        "# Repeat again for the DenseNet121 model, targeting its last convolutional layer.\n",
        "cam_obj = GradCAM(model=densenet121, target_layer=densenet121.features[-1])\n",
        "_, dense_cam = cam_obj(image, label)\n",
        "\n",
        "# Utilize a custom function to plot the Grad-CAM visualizations for the selected image.\n",
        "# This visual comparison helps in interpreting the focus areas of different models for the given X-ray image.\n",
        "plot_gradcam(image, label_name, vgg_cam, res_cam, dense_cam)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "W5BPZVfYbBdf",
        "outputId": "9708dddf-6c3e-4790-8313-61212ef76493"
      },
      "outputs": [],
      "source": [
        "# Select an X-ray image and its true label from the test set using an index.\n",
        "# The chosen image will be used for visualization of class activation maps (CAMs).\n",
        "index = 88\n",
        "image, label = test_set[index]\n",
        "\n",
        "# Retrieve the class names corresponding to the true label and the predicted labels from each model\n",
        "# for the selected image. This aids in understanding the model's predictions in human-readable terms.\n",
        "label_name = [\n",
        "    test_set.classes[label],  # True label\n",
        "    test_set.classes[vgg_preds[index]],  # Prediction from VGG16\n",
        "    test_set.classes[resnet_preds[index]],  # Prediction from ResNet18\n",
        "    test_set.classes[densenet_preds[index]]  # Prediction from DenseNet121\n",
        "]\n",
        "\n",
        "# Prepare the image for the model by adding a batch dimension and transferring it to the same device as the model.\n",
        "image = image.unsqueeze(dim=0).to(device)\n",
        "\n",
        "# Initialize the Grad-CAM object for VGG16 and generate the CAM for the selected image and its true label.\n",
        "# The target layer is the final convolutional layer of the VGG16 model.\n",
        "cam_obj = GradCAM(model=vgg16, target_layer=vgg16.features[-1])\n",
        "_, vgg_cam = cam_obj(image, label)\n",
        "\n",
        "# Repeat the process with the ResNet18 model, targeting the final layer of the model's fourth layer block.\n",
        "cam_obj = GradCAM(model=resnet18, target_layer=resnet18.layer4[-1])\n",
        "_, res_cam = cam_obj(image, label)\n",
        "\n",
        "# Repeat again for the DenseNet121 model, targeting its last convolutional layer.\n",
        "cam_obj = GradCAM(model=densenet121, target_layer=densenet121.features[-1])\n",
        "_, dense_cam = cam_obj(image, label)\n",
        "\n",
        "# Utilize a custom function to plot the Grad-CAM visualizations for the selected image.\n",
        "# This visual comparison helps in interpreting the focus areas of different models for the given X-ray image.\n",
        "plot_gradcam(image, label_name, \"fill here\", \"fill here\", \"fill here\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6FCI4ofdKax"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
