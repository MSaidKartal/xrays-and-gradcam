{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "resnet18.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YTsvV6QIMZ-"
      },
      "source": [
        "# !touch __init__.py\n",
        "# !cp 'drive/MyDrive/COVID-19_Radiography_Database.zip' .\n",
        "# !unzip COVID-19_Radiography_Database.zip\n",
        "# !rm COVID-19_Radiography_Database.zip\n",
        "# !find \"/content/COVID-19_Radiography_Dataset\" -type f | wc -l  # 21170"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58Ce5jsBKRE4"
      },
      "source": [
        "# !python split_dataset.py"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgZk4nz3LuFd"
      },
      "source": [
        "# !find \"/content/COVID-19_Radiography_Dataset\" -type f | wc -l  # 21170\n",
        "# !zip -r temp.zip \"COVID-19_Radiography_Dataset/\"\n",
        "# !mv temp.zip drive/MyDrive/"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6Lx3muqILWw"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXVJZO0cILXA",
        "outputId": "6e075439-d79b-4946-9721-fb3aa28976e8"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oMJyVNkILXB"
      },
      "source": [
        "transform = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD9VVf7tILXC"
      },
      "source": [
        "dirs = {\n",
        "    'train': 'data/COVID-19_Radiography_Dataset/train',\n",
        "    'val': 'data/COVID-19_Radiography_Dataset/val'\n",
        "}\n",
        "train_set = datasets.ImageFolder(root=dirs['train'], transform=transform['train'])\n",
        "val_set = datasets.ImageFolder(root=dirs['val'], transform=transform['val'])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9y7jDG-ILXD"
      },
      "source": [
        "class_freq = torch.as_tensor(train_set.targets).bincount()\n",
        "weight = 1 / class_freq\n",
        "samples_weight = weight[train_set.targets]\n",
        "sampler = WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=16, sampler=sampler)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=32)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZMY9Oh0Xwk0",
        "outputId": "1e4f80f1-bb6e-4777-edbb-5cf829fd449d"
      },
      "source": [
        "# count = dict((c, 0) for c in train_set.classes)\n",
        "# idx_to_class = dict((v, k) for (k, v) in train_set.class_to_idx.items())\n",
        "# for _, l in torch.utils.data.DataLoader(train_set, sampler=sampler):\n",
        "#     count[idx_to_class[l.item()]] += 1\n",
        "\n",
        "# count"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'covid_19': 5270, 'lung_opacity': 5235, 'normal': 5256, 'pneumonia': 5244}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frg7rvsUILXF",
        "outputId": "a754461e-aaa6-48a3-e194-ade73bcb195a"
      },
      "source": [
        "resnet18 = torchvision.models.resnet18(pretrained=True)\n",
        "resnet18.fc = nn.Linear(in_features=512, out_features=4)\n",
        "resnet18.to(device)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42vwTgwsILXF"
      },
      "source": [
        "def get_num_correct(preds, labels):\n",
        "    return preds.argmax(dim=1).eq(labels).sum().item()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoH23wktILXG",
        "outputId": "f2bb6159-0751-4a9d-ba89-b9d5149a41a3"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(resnet18.parameters(), lr=3e-5)\n",
        "\n",
        "val_loss_min = np.Inf\n",
        "num_epochs = 10\n",
        "\n",
        "len_train = len(train_set)\n",
        "len_val = len(val_set)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_correct = 0, 0\n",
        "    train_loop = tqdm(train_loader)\n",
        "    resnet18.train()\n",
        "\n",
        "    for batch in train_loop:\n",
        "        images, labels = batch[0].to(device), batch[1].to(device)\n",
        "        preds = resnet18(images)\n",
        "        loss = criterion(preds, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * labels.size(0)\n",
        "        train_correct += get_num_correct(preds, labels)\n",
        "\n",
        "        train_loop.set_description(f'Epoch [{epoch+1:2d}/{num_epochs}]')\n",
        "        train_loop.set_postfix(loss=loss.item(), acc=train_correct/len_train)\n",
        "\n",
        "\n",
        "    resnet18.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        val_loss = 0\n",
        "        for batch in val_loader:\n",
        "            images, labels = batch[0].to(device), batch[1].to(device)\n",
        "            preds = resnet18(images)\n",
        "            loss = criterion(preds, labels)\n",
        "            val_loss += loss.item() * labels.size(0)\n",
        "\n",
        "        train_loss = train_loss/len_train\n",
        "        val_loss = val_loss/len_val\n",
        "        train_loop.write(f'\\t\\tAvg training loss: {train_loss:.6f}\\tAvg validation loss: {val_loss:.6f}')\n",
        "\n",
        "        # save model if validation loss has decreased\n",
        "        if val_loss <= val_loss_min:\n",
        "            train_loop.write(f'\\t\\tval_loss decreased ({val_loss_min:.6f} --> {val_loss:.6f})  saving model...')\n",
        "            torch.save(resnet18.state_dict(), 'models/lr3e-5_resnet18_gpu.pth')\n",
        "            val_loss_min = val_loss"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [ 1/10]: 100%|██████████| 1313/1313 [04:28<00:00,  4.89it/s, acc=0.912, loss=0.12]\n",
            "  0%|          | 0/1313 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.244246\tAvg validation loss: 0.056423\n",
            "\t\tval_loss decreased (inf --> 0.056423)  saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [ 2/10]: 100%|██████████| 1313/1313 [04:28<00:00,  4.89it/s, acc=0.956, loss=0.0205]\n",
            "  0%|          | 0/1313 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.126862\tAvg validation loss: 0.049716\n",
            "\t\tval_loss decreased (0.056423 --> 0.049716)  saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [ 3/10]: 100%|██████████| 1313/1313 [04:28<00:00,  4.89it/s, acc=0.967, loss=0.0337]\n",
            "  0%|          | 0/1313 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.096389\tAvg validation loss: 0.039880\n",
            "\t\tval_loss decreased (0.049716 --> 0.039880)  saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [ 4/10]: 100%|██████████| 1313/1313 [04:29<00:00,  4.87it/s, acc=0.975, loss=0.00805]\n",
            "  0%|          | 0/1313 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.073555\tAvg validation loss: 0.026281\n",
            "\t\tval_loss decreased (0.039880 --> 0.026281)  saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [ 5/10]: 100%|██████████| 1313/1313 [04:29<00:00,  4.87it/s, acc=0.98, loss=0.0165]\n",
            "Epoch [ 6/10]:   0%|          | 0/1313 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.063953\tAvg validation loss: 0.032789\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [ 6/10]: 100%|██████████| 1313/1313 [04:29<00:00,  4.86it/s, acc=0.98, loss=0.026]\n",
            "  0%|          | 0/1313 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.058138\tAvg validation loss: 0.043833\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [ 7/10]: 100%|██████████| 1313/1313 [04:30<00:00,  4.85it/s, acc=0.984, loss=0.028]\n",
            "  0%|          | 0/1313 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.048760\tAvg validation loss: 0.027824\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [ 8/10]: 100%|██████████| 1313/1313 [04:30<00:00,  4.85it/s, acc=0.986, loss=0.00897]\n",
            "  0%|          | 0/1313 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.040949\tAvg validation loss: 0.022712\n",
            "\t\tval_loss decreased (0.026281 --> 0.022712)  saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [ 9/10]: 100%|██████████| 1313/1313 [04:30<00:00,  4.86it/s, acc=0.988, loss=0.000634]\n",
            "  0%|          | 0/1313 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.036118\tAvg validation loss: 0.035958\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [10/10]: 100%|██████████| 1313/1313 [04:29<00:00,  4.88it/s, acc=0.99, loss=0.0028]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.031628\tAvg validation loss: 0.016979\n",
            "\t\tval_loss decreased (0.022712 --> 0.016979)  saving model...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}